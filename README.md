# MCP Basic - Learning MCP with Local LLM

A complete implementation of Model Context Protocol (MCP) using local LLM with Ollama. This project showcases the full MCP architecture with proper client-server communication via JSON-RPC over STDIO - perfect for learning MCP fundamentals without API keys, runs entirely on your machine.

**Repository**: https://github.com/baliqbalpe/mcp-basic

---

## üèóÔ∏è Architecture

This project implements the **full MCP protocol specification**:

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    User                          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                      ‚îÇ
                      ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ               CLI (cli.py)                       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                      ‚îÇ
                      ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         MCP Client (mcp_client.py)               ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ  - Spawns MCP Server subprocess          ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  - Connects via STDIO                    ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  - Discovers tools (list_tools)          ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  - Executes tools (call_tool)            ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  - Integrates with Ollama for chat       ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
            ‚îÇ                  ‚îÇ
            ‚îÇ JSON-RPC         ‚îÇ HTTP
            ‚îÇ over STDIO       ‚îÇ
            ‚ñº                  ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   MCP Server      ‚îÇ   ‚îÇ   Ollama LLM     ‚îÇ
‚îÇ (mcp_server.py)   ‚îÇ   ‚îÇ  (localhost)     ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
‚îÇ  ‚îÇlist_tools() ‚îÇ  ‚îÇ
‚îÇ  ‚îÇcall_tool()  ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   MCPTools        ‚îÇ
‚îÇ   (tools.py)      ‚îÇ
‚îÇ  - calculator     ‚îÇ
‚îÇ  - list_files     ‚îÇ
‚îÇ  - read_file      ‚îÇ
‚îÇ  - etc.           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Key Components:

1. **MCP Client** - Acts as the MCP Host, spawns server, discovers tools dynamically
2. **MCP Server** - Standalone subprocess exposing tools via JSON-RPC over STDIO
3. **Transport Layer** - STDIO communication (standard MCP transport)
4. **Protocol** - JSON-RPC 2.0 with MCP message types
5. **LLM Integration** - Ollama for natural language interaction

---

## üöÄ Quick Start

```bash
# 1. Clone the repository
git clone https://github.com/baliqbalpe/mcp-basic.git
cd mcp-basic

# 2. Run automated setup
chmod +x setup.sh start.sh
./setup.sh

# 3. Start chatting!
./start.sh
```

That's it! The setup script installs everything automatically.

---

## üåü Features

- ‚úÖ **Full MCP Protocol** - Complete JSON-RPC 2.0 over STDIO implementation
- ‚úÖ **Client-Server Architecture** - Proper separation with subprocess communication
- ‚úÖ **Dynamic Tool Discovery** - Tools discovered at runtime via `list_tools()`
- ‚úÖ **8 Built-in Tools** - File operations, calculations, system info
- ‚úÖ **Local LLM** - Runs Ollama models (Llama 3.2, Mistral) locally
- ‚úÖ **Interactive CLI** - Beautiful terminal interface
- ‚úÖ **No API Keys** - Completely private, runs offline
- ‚úÖ **Cloud Ready** - Easy deployment on any Linux server
- ‚úÖ **MCP Protocol** - Full Model Context Protocol implementation

---

## üõ†Ô∏è Available Tools

| Tool | Description |
|------|-------------|
| `calculator` | Evaluate math expressions |
| `get_current_time` | Get current date/time with timezone |
| `list_files` | List files and directories |
| `read_file` | Read file contents |
| `write_file` | Write content to files |
| `system_info` | Get OS, CPU, Python version |
| `execute_command` | Run shell commands |

---

## üìã Prerequisites

- **Python 3.8+** (3.10+ recommended)
- **Linux or macOS** (Windows with WSL2)
- **4GB RAM minimum** (8GB recommended)
- **10GB disk space** (for models)

---

## üì¶ Tested Versions

This project has been tested with the following dependency versions to ensure stability:

### Core Dependencies
- **Python**: 3.8+ (recommended 3.10+)
- **pip**: 24.2
- **Ollama**: 0.3.x or later

### Python Packages (pinned in requirements.txt)
- **mcp**: 1.1.2
- **httpx**: 0.27.2
- **aiohttp**: 3.10.5
- **ollama**: 0.3.3
- **python-dotenv**: 1.0.1
- **click**: 8.1.7
- **rich**: 13.9.2
- **pydantic**: 2.9.2

**Last tested**: October 2025

> **Note**: All Python package versions are pinned in `requirements.txt` to prevent breaking changes. The setup script will install pip 24.2 and Ollama's latest stable version.

---

## üíª Installation

```bash
git clone https://github.com/baliqbalpe/mcp-basic.git
cd mcp-basic
chmod +x setup.sh
./setup.sh
```

---

## üí¨ Usage

### Starting the Client

```bash
./start.sh
```

Or manually:

```bash
source venv/bin/activate
python3 main.py
```

### Example Interactions

**Calculate Math:**
```
You: What is 15 * 23 + 100?
[Calling tool: calculator]
Assistant: The result is 445.
```

**Get Time:**
```
You: What time is it?
[Calling tool: get_current_time]
Assistant: It's currently 14:30:45 on Sunday, October 12, 2025.
```

**List Files:**
```
You: List files in the current directory
[Calling tool: list_files]
Assistant: Here are the files: main.py, requirements.txt, src/, data/...
```

**System Info:**
```
You: What system am I on?
[Calling tool: system_info]
Assistant: You're running Linux with Python 3.10.12...
```

### CLI Commands

- `exit` or `quit` - Exit the application
- `reset` - Clear conversation history
- `Ctrl+C` - Interrupt current operation

---

## ‚öôÔ∏è Configuration

Edit `.env` file to customize:

```bash
# Ollama Configuration
OLLAMA_HOST=http://localhost:11434
OLLAMA_MODEL=llama3.2:3b

# Logging
LOG_LEVEL=INFO
```

### Available Models

```bash
# Smallest (2GB RAM)
ollama pull llama3.2:1b

# Recommended (4GB RAM)
ollama pull llama3.2:3b

# Better quality (8GB RAM)
ollama pull llama3:8b

# Alternative
ollama pull mistral:latest

# Update .env with your choice
nano .env  # Change OLLAMA_MODEL=your-model
```

---

## üìù Command Reference

### Daily Use

```bash
# Start everything
./start.sh

# Or manually
cd /root/mcp-basic
source venv/bin/activate
python3 main.py
```

### Ollama Management

```bash
# Check if running
pgrep -x ollama

# Start Ollama
nohup ollama serve > ollama.log 2>&1 &

# Stop Ollama
pkill ollama

# List models
ollama list

# Pull a model
ollama pull llama3.2:3b

# Check logs
tail -f ollama.log
```

### Testing

```bash
# Test tools
python3 test_server.py

# Test Ollama
curl http://localhost:11434/api/tags
```

### Background Running

```bash
# Using tmux (recommended)
apt install -y tmux
tmux new -s mcp
python3 main.py
# Detach: Ctrl+B then D
# Reattach: tmux attach -t mcp

# Using screen
apt install -y screen
screen -S mcp
python3 main.py
# Detach: Ctrl+A then D
# Reattach: screen -r mcp
```

---

## üîß Troubleshooting

### MCP Server Connection Issues

```bash
# Check if server can run standalone
python3 src/mcp_server.py
# Should show: "Starting MCP Server" and wait for connections

# Check logs
# Look for "Client connected via STDIO" in logs
```

### Ollama Not Running

```bash
# Check status
pgrep -x ollama

# Start it
nohup ollama serve > ollama.log 2>&1 &
sleep 3
```

### Model Not Found

```bash
# Check installed models
ollama list

# Pull again
ollama pull llama3.2:3b
```

### Import Errors

```bash
# Check you're in project directory
pwd  # Should show: /root/mcp-basic

# Check venv is activated
which python3  # Should show: .../venv/bin/python3

# Reinstall dependencies
python3 -m pip install --force-reinstall -r requirements.txt
```

### Out of Memory

```bash
# Check memory
free -h

# Use smaller model
ollama pull llama3.2:1b
nano .env  # Change to OLLAMA_MODEL=llama3.2:1b
```

### After Reboot

```bash
cd /root/mcp-basic
source venv/bin/activate
nohup ollama serve > ollama.log 2>&1 &
sleep 3
python3 main.py
```

---

## üìä System Requirements

### Minimum
- 2 CPU cores
- 4GB RAM
- 10GB disk space

### Recommended
- 4+ CPU cores
- 8GB+ RAM
- 20GB+ disk space

### Model Size Guide

| Model | RAM | Speed | Quality |
|-------|-----|-------|---------|
| llama3.2:1b | 2GB | Very Fast | Basic |
| llama3.2:3b | 4GB | Fast | Good |
| llama3:8b | 8GB | Medium | Excellent |
| mistral:latest | 6GB | Medium | Very Good |

---

## üìÅ Project Structure

```
mcp-basic/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ config.py          # Configuration
‚îÇ   ‚îú‚îÄ‚îÄ tools.py           # MCP tools implementation
‚îÇ   ‚îú‚îÄ‚îÄ mcp_server.py      # MCP Server (STDIO, JSON-RPC)
‚îÇ   ‚îú‚îÄ‚îÄ mcp_client.py      # MCP Client (spawns server, Ollama integration)
‚îÇ   ‚îî‚îÄ‚îÄ cli.py             # Interactive CLI
‚îú‚îÄ‚îÄ data/                  # Data directory
‚îú‚îÄ‚îÄ main.py                # Entry point
‚îú‚îÄ‚îÄ setup.sh               # Setup script
‚îú‚îÄ‚îÄ start.sh               # Start script
‚îú‚îÄ‚îÄ test_server.py         # Testing script
‚îú‚îÄ‚îÄ requirements.txt       # Dependencies
‚îú‚îÄ‚îÄ env.example            # Config template
‚îî‚îÄ‚îÄ README.md              # This file
```

---

## üîê Security Notes

- The `execute_command` tool can run shell commands - use carefully
- When deploying to cloud, configure firewall rules
- Consider removing dangerous tools in production
- Never expose MCP server to internet without authentication

---

## üõ†Ô∏è Development

### Adding New Tools

1. Add tool method to `MCPTools` class in `src/tools.py`
2. Add tool definition to `TOOL_DEFINITIONS` in `src/tools.py`
3. Add routing in `src/mcp_server.py` `call_tool()` handler

**That's it!** The client will automatically discover the new tool via the MCP protocol.

Example:

```python
# In src/tools.py
@staticmethod
def my_tool(param: str) -> Dict[str, Any]:
    """Tool description."""
    return {"success": True, "result": param.upper()}

# Add to TOOL_DEFINITIONS
{
    "name": "my_tool",
    "description": "What the tool does",
    "inputSchema": {
        "type": "object",
        "properties": {
            "param": {"type": "string", "description": "Parameter description"}
        },
        "required": ["param"]
    }
}

# In src/mcp_server.py call_tool()
elif name == "my_tool":
    result = self.tools.my_tool(arguments.get("param", ""))
```

The client will automatically discover this new tool on next startup!

### Testing

```bash
# Test tools
python3 test_server.py

# Test in client
python3 main.py
```

---

## üìö Resources

- [Model Context Protocol Documentation](https://modelcontextprotocol.io)
- [Ollama Documentation](https://ollama.com/docs)
- [Ollama Model Library](https://ollama.com/library)

---

## üí° Tips

1. Start with `llama3.2:3b` model for best balance
2. Use `reset` command to clear context and save memory
3. More detailed tool descriptions = better tool usage
4. Monitor resources with `htop` when running
5. Use tmux/screen for persistent sessions on cloud
6. The MCP server runs as a subprocess - you'll see "Starting MCP server subprocess" on startup
7. Tools are discovered dynamically - no need to restart client when adding tools to server

---

## üìÑ License

This project is open source and available for use and modification.

---

## ü§ù Contributing

Feel free to:
- Submit issues
- Fork the repository
- Create pull requests
- Suggest new tools or features

---

**Built with ‚ù§Ô∏è using Model Context Protocol and Ollama**

Questions? Open an issue on [GitHub](https://github.com/baliqbalpe/mcp-basic)

